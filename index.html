<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<title>MLiature</title>
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i,900,900i">
        <link rel="stylesheet" href="reset.css">
		<link rel="stylesheet" href="article.css">
		<link rel="stylesheet" href="article-figure.css">
		<link rel="stylesheet" href="article-text.css">
		<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>


<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>

	</head>
    <body>
		<article id="header-1">
			<h1><a>This site showcases The <em>AI Capstone Project</em></a></h1>
			<h2>Session 2024-25</h2>

			<time datetime="12-07-2024">JULY. 12, 2024</time>
			<p>
				By Tejas,Akshay,Shreya,Satvik,Sparsh and Prince.<br>
				Class  12 C2
			</p>
			
			<figure class="size-4">
				 <canvas id="canvas"></canvas>
                    <video id="video" muted="muted" playsinline="" style="transform:scaleX(-1);visibility:hidden;width:auto;height:auto;"></video>
			</figure>
			<figcaption><p>Pose Estimation</p></figcaption>

			<h3>About the project</h3>
			<p>
				MLiature is a portmanteau of Machine Learning(ML) and Miniature paintings(-iature).
				The project symbolizes the rich cultural heritage of India
				in the aspect of art.<br>
				It is backed by a realtime pose estimation model trained by google,MoveNet Lightning.
			</p>
			<footer>
			  <p><a href="logbook.docx">See the project logbook</a></p>
			  <p>Team Email: <a href="mailto:mliatureaiprojecttps@gmail.com">mliatureaiprojecttps@gmail.com</a></p>
			</footer>
		</article>
    </body>

		<script>/*

const video = document.getElementById('video');

window.requestAnimationFrame(async function(){
  
  output.width = video.videoWidth;
  output.height = video.videoHeight;
  ctx.drawImage(video, 0, 0);
});
*/
const detectorConfig = {modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING};
let detector;


const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
let frame = 0;
const ctx = canvas.getContext('2d');
if ('mediaDevices' in navigator && 'getUserMedia' in navigator.mediaDevices) {
  console.log("mediaDevices found in navigator.getUserMedia found in mediaDevices")
}
async function cameraToVideo (){
const stream = await navigator.mediaDevices.getUserMedia({video: true});
let poses;
video.srcObject = stream;
}
async function videoToCanvas(){

canvas.width = video.videoWidth;
canvas.height = video.videoHeight;
//if( frame%3==0){
 poses = await detector.estimatePoses(video);
//}
if (frame<60){
frame +=1;
}else{
frame=0;
}

   ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.scale(-1, 1);
    ctx.translate(-canvas.width, 0);
    ctx.drawImage(video, 0, 0);
for(let i=5;i<16;i++){
if (poses[0]){
ctx.globalAlpha = 0.5;
ctx.fillStyle = "rgb(255,255,255)";
ctx.beginPath();
ctx.arc(poses[0]['keypoints'][i]['x'],poses[0]['keypoints'][i]['y'],8,0,2*Math.PI);
ctx.fill()

}
console.log(poses[0]);

}

    ctx.setTransform(1, 0, 0, 1, 0, 0);

requestAnimationFrame(videoToCanvas);
}
async function initialise(){
detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);
requestAnimationFrame(videoToCanvas);

}
video.play()
cameraToVideo();
initialise();
</script>

</html>
