<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<title>MLiature</title>
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i,900,900i">
        <link rel="stylesheet" href="reset.css">
		<link rel="stylesheet" href="article.css">
		<link rel="stylesheet" href="article-figure.css">
		<link rel="stylesheet" href="article-text.css">
		<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>


<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>
	</head>
    <body>
		<article id="header-1">
			<h1><a>This site showcases The <em>AI Capstone Project</em></a></h1>
			<h2>Session 2024-25</h2>

			<time datetime="12-07-2024">JULY. 12, 2024</time>
			<p>
				By Tejas,Akshay,Shreya,Satvik,Sparsh and Prince.<br>
				Class  12 C2
			</p>
			
			<figure class="size-4">
				 <canvas id="output"></canvas>
                    <video id="video" muted="muted" playsinline="" style="-webkit-transform:scaleX(-1);transform:scaleX(-1);;width:auto;height:auto;"></video>

		<script>/*
const detectorConfig = {modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING};

const video = document.getElementById('video');
const output = document.getElementById('output');
let ctx = output.getContext('2d');


const constraints = {
  audio: false,
  video: true
};

function handleSuccess(stream) {
  window.stream = stream; // make stream available to browser console
  video.srcObject = stream;
}

function handleError(error) {
  console.log('navigator.MediaDevices.getUserMedia error: ', error.message, error.name);
}

navigator.mediaDevices.getUserMedia(constraints).then(handleSuccess).catch(handleError);
window.requestAnimationFrame(async function(){
  const detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);
const poses = await detector.estimatePoses(video);
  output.width = video.videoWidth;
  output.height = video.videoHeight;
  ctx.drawImage(video, 0, 0);
});
*/
let video = document.getElementById('video');
if ('mediaDevices' in navigator && 'getUserMedia' in navigator.mediaDevices) {
  console.log("mediaDevices found in navigator.getUserMedia found in mediaDevices")
}
async function cameraToVideo (){
const stream = await navigator.mediaDevices.getUserMedia({video: true});
video.srcObject = stream;
}
video.play()
cameraToVideo();
</script>

			</figure>
			<figcaption><p>Pose Estimation</p></figcaption>

			<h3>About the project</h3>
			<p>
				MLiature is a portmanteau of Machine Learning(ML) and Miniature paintings(-iature).
				The project symbolizes the rich cultural heritage of India
				in the aspect of art.<br>
				It is backed by a realtime pose estimation model trained by google,MoveNet Lightning.
			</p>
			<footer>
			  <p><a href="logbook.docx">See the project logbook</a></p>
			  <p>Team Email: <a href="mailto:mliatureaiprojecttps@gmail.com">mliatureaiprojecttps@gmail.com</a></p>
			</footer>
		</article>
    </body>
</html>
